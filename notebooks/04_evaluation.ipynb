{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Benchmarks & Evaluation\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-org/neurosym-kg/blob/main/notebooks/04_evaluation.ipynb)\n",
    "\n",
    "This notebook demonstrates how to evaluate reasoners on standard benchmarks:\n",
    "\n",
    "- **WebQSP** - WebQuestions Semantic Parsing\n",
    "- **CWQ** - ComplexWebQuestions\n",
    "- **MetaQA** - Multi-hop movie QA\n",
    "- **SimpleQuestions** - Single-relation questions\n",
    "\n",
    "**Note**: For full benchmark evaluation, you'll need to download the datasets. We provide demo examples for quick testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q pydantic pydantic-settings httpx tenacity networkx numpy tqdm\n",
    "\n",
    "# For Colab Pro with GPU (if using HuggingFace models)\n",
    "# !pip install -q torch transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from neurosym_kg import (\n",
    "    InMemoryKG,\n",
    "    Triple,\n",
    "    MockLLMBackend,\n",
    "    ThinkOnGraphReasoner,\n",
    ")\n",
    "from neurosym_kg.evaluation import (\n",
    "    WebQSP,\n",
    "    CWQ,\n",
    "    MetaQA,\n",
    "    BenchmarkRunner,\n",
    "    RunConfig,\n",
    "    MetricsCalculator,\n",
    "    exact_match,\n",
    "    f1_score,\n",
    "    normalize_answer,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore Available Benchmarks\n",
    "\n",
    "NeuroSym-KG includes demo examples for each benchmark. For full evaluation, download the actual datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load benchmarks (uses built-in demo examples)\n",
    "benchmarks = {\n",
    "    \"WebQSP\": WebQSP(),\n",
    "    \"CWQ\": CWQ(),\n",
    "    \"MetaQA-1hop\": MetaQA(hops=1),\n",
    "    \"MetaQA-2hop\": MetaQA(hops=2),\n",
    "}\n",
    "\n",
    "print(\"üìö Available Benchmarks:\")\n",
    "print(\"=\" * 60)\n",
    "for name, bench in benchmarks.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Examples: {len(bench)}\")\n",
    "    \n",
    "    # Show sample questions\n",
    "    for ex in list(bench)[:2]:\n",
    "        print(f\"  ‚Ä¢ Q: {ex.question[:50]}...\")\n",
    "        print(f\"    A: {ex.answers[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate metric calculations\n",
    "print(\"üìè Metric Examples:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_cases = [\n",
    "    (\"Paris\", \"Paris\"),\n",
    "    (\"PARIS\", \"Paris\"),\n",
    "    (\"The Eiffel Tower\", \"Eiffel Tower\"),\n",
    "    (\"Christopher Nolan\", \"Nolan\"),\n",
    "    (\"London, UK\", \"London\"),\n",
    "    (\"wrong answer\", \"correct answer\"),\n",
    "]\n",
    "\n",
    "print(f\"{'Prediction':<25} {'Ground Truth':<20} {'EM':>6} {'F1':>6}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for pred, gt in test_cases:\n",
    "    em = exact_match(pred, gt)\n",
    "    f1 = f1_score(pred, gt)\n",
    "    print(f\"{pred:<25} {gt:<20} {em:>6.2f} {f1:>6.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer normalization\n",
    "print(\"\\nüîÑ Answer Normalization:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "examples = [\n",
    "    \"The United States of America\",\n",
    "    \"Christopher Nolan\",\n",
    "    \"  PARIS, FRANCE  \",\n",
    "]\n",
    "\n",
    "for ex in examples:\n",
    "    normalized = normalize_answer(ex)\n",
    "    print(f\"'{ex}' ‚Üí '{normalized}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Test Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_demo_kg() -> InMemoryKG:\n",
    "    \"\"\"Create a KG that can answer demo benchmark questions.\"\"\"\n",
    "    kg = InMemoryKG(name=\"Demo KG\")\n",
    "    \n",
    "    # Facts for WebQSP demo questions\n",
    "    kg.add_triples([\n",
    "        Triple(\"France\", \"capital\", \"Paris\"),\n",
    "        Triple(\"Paris\", \"located_in\", \"France\"),\n",
    "        Triple(\"Paris\", \"type\", \"City\"),\n",
    "        \n",
    "        Triple(\"Inception\", \"director\", \"Christopher_Nolan\"),\n",
    "        Triple(\"Christopher_Nolan\", \"born_in\", \"London\"),\n",
    "        Triple(\"Christopher_Nolan\", \"nationality\", \"British\"),\n",
    "        \n",
    "        Triple(\"Eiffel_Tower\", \"located_in\", \"Paris\"),\n",
    "        Triple(\"Paris\", \"country\", \"France\"),\n",
    "        Triple(\"France\", \"official_language\", \"French\"),\n",
    "        \n",
    "        Triple(\"Barack_Obama\", \"spouse\", \"Michelle_Obama\"),\n",
    "        Triple(\"Michelle_Obama\", \"spouse\", \"Barack_Obama\"),\n",
    "        \n",
    "        Triple(\"Tesla\", \"CEO\", \"Elon_Musk\"),\n",
    "        Triple(\"Elon_Musk\", \"education\", \"University_of_Pennsylvania\"),\n",
    "    ])\n",
    "    \n",
    "    return kg\n",
    "\n",
    "kg = create_demo_kg()\n",
    "print(kg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure Reasoner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock LLM with generic responses\n",
    "llm = MockLLMBackend()\n",
    "\n",
    "# Generic patterns for entity extraction\n",
    "llm.add_response(r\".*Extract.*France.*\", \"France\")\n",
    "llm.add_response(r\".*Extract.*Inception.*\", \"Inception\")\n",
    "llm.add_response(r\".*Extract.*Obama.*\", \"Barack_Obama\")\n",
    "llm.add_response(r\".*Extract.*Eiffel.*\", \"Eiffel_Tower\")\n",
    "llm.add_response(r\".*Extract.*Tesla.*\", \"Tesla\")\n",
    "llm.add_response(r\".*Extract.*Nolan.*\", \"Christopher_Nolan\")\n",
    "llm.add_response(r\".*Extract.*\", \"Unknown_Entity\")  # Fallback\n",
    "\n",
    "# Generic patterns\n",
    "llm.add_response(r\".*relevant.*\", \"capital\\nlocated_in\\ndirector\\nspouse\")\n",
    "llm.add_response(r\".*enough.*\", \"YES\")\n",
    "llm.add_response(r\".*answer.*capital.*France.*\", \"Paris\")\n",
    "llm.add_response(r\".*answer.*director.*Inception.*\", \"Christopher Nolan\")\n",
    "llm.add_response(r\".*answer.*spouse.*Obama.*\", \"Michelle Obama\")\n",
    "llm.add_response(r\".*answer.*\", \"Unknown\")  # Fallback\n",
    "\n",
    "# Create reasoner\n",
    "reasoner = ThinkOnGraphReasoner(\n",
    "    kg=kg,\n",
    "    llm=llm,\n",
    "    max_depth=2,\n",
    "    beam_width=3,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Reasoner configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Evaluation with BenchmarkRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create benchmark runner\n",
    "runner = BenchmarkRunner(reasoner)\n",
    "\n",
    "# Configure evaluation\n",
    "config = RunConfig(\n",
    "    subset_size=5,      # Evaluate on 5 examples (use None for full dataset)\n",
    "    random_seed=42,     # For reproducibility\n",
    "    max_retries=1,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(\"üìä Running WebQSP Evaluation...\")\n",
    "report = runner.evaluate(WebQSP(), config=config)\n",
    "\n",
    "# Print summary\n",
    "print(report.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed results\n",
    "print(\"\\nüìã Per-Question Results:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for pred in report.results.predictions:\n",
    "    status = \"‚úÖ\" if pred.exact_match else \"‚ùå\"\n",
    "    print(f\"\\n{status} Q: {pred.question}\")\n",
    "    print(f\"   Predicted: {pred.prediction}\")\n",
    "    print(f\"   Expected: {pred.ground_truth}\")\n",
    "    print(f\"   EM: {pred.exact_match:.0f}, F1: {pred.f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Multiple Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on multiple benchmarks\n",
    "benchmark_results = {}\n",
    "\n",
    "for bench_name, benchmark in benchmarks.items():\n",
    "    print(f\"\\nüìä Evaluating on {bench_name}...\")\n",
    "    \n",
    "    try:\n",
    "        report = runner.evaluate(benchmark, config=config)\n",
    "        benchmark_results[bench_name] = report\n",
    "        print(f\"   Accuracy: {report.results.accuracy:.1%}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"                  BENCHMARK RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Benchmark':<20} {'Accuracy':>10} {'F1':>10} {'Samples':>10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, report in benchmark_results.items():\n",
    "    metrics = report.results.metrics\n",
    "    print(f\"{name:<20} {metrics['accuracy']:>10.1%} {metrics['f1']:>10.1%} {int(metrics['num_samples']):>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Manual Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For more control, use MetricsCalculator directly\n",
    "calculator = MetricsCalculator(dataset_name=\"Custom\")\n",
    "\n",
    "custom_questions = [\n",
    "    {\"q\": \"What is the capital of France?\", \"a\": [\"Paris\"]},\n",
    "    {\"q\": \"Who directed Inception?\", \"a\": [\"Christopher Nolan\"]},\n",
    "    {\"q\": \"Who is Obama's spouse?\", \"a\": [\"Michelle Obama\"]},\n",
    "]\n",
    "\n",
    "print(\"Running custom evaluation...\\n\")\n",
    "\n",
    "for item in custom_questions:\n",
    "    result = reasoner.reason(item[\"q\"])\n",
    "    \n",
    "    calculator.add_prediction(\n",
    "        prediction=result.answer,\n",
    "        ground_truth=item[\"a\"],\n",
    "        question=item[\"q\"],\n",
    "        confidence=result.confidence,\n",
    "        latency_ms=result.latency_ms,\n",
    "    )\n",
    "    \n",
    "    em = exact_match(result.answer, item[\"a\"])\n",
    "    status = \"‚úÖ\" if em else \"‚ùå\"\n",
    "    print(f\"{status} {item['q']}\")\n",
    "    print(f\"   ‚Üí {result.answer}\")\n",
    "\n",
    "# Get aggregated results\n",
    "final_results = calculator.compute()\n",
    "print(f\"\\nFinal Accuracy: {final_results.accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze errors from the evaluation\n",
    "if benchmark_results:\n",
    "    report = list(benchmark_results.values())[0]\n",
    "    \n",
    "    print(\"üîç Error Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    errors = report.results.error_analysis(top_n=5)\n",
    "    \n",
    "    if errors:\n",
    "        print(f\"\\nTop {len(errors)} errors (by confidence):\")\n",
    "        for i, err in enumerate(errors, 1):\n",
    "            print(f\"\\n{i}. Q: {err.question}\")\n",
    "            print(f\"   Predicted: {err.prediction}\")\n",
    "            print(f\"   Expected: {err.ground_truth}\")\n",
    "            print(f\"   Confidence: {err.confidence:.2f} (high confidence = more problematic)\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No errors found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation report\n",
    "if benchmark_results:\n",
    "    report = list(benchmark_results.values())[0]\n",
    "    \n",
    "    # Save to JSON\n",
    "    output_path = \"evaluation_results.json\"\n",
    "    report.save(output_path)\n",
    "    print(f\"‚úÖ Results saved to {output_path}\")\n",
    "    \n",
    "    # Preview saved content\n",
    "    with open(output_path) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(\"\\nSaved data preview:\")\n",
    "    print(json.dumps({k: v for k, v in data.items() if k != 'predictions'}, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Running Full Benchmarks\n",
    "\n",
    "To run on the complete benchmarks (not just demo examples):\n",
    "\n",
    "### Download Datasets\n",
    "\n",
    "```bash\n",
    "# WebQSP\n",
    "wget https://download.microsoft.com/download/0/7/5/0755490B-8F8F-4DB3-9B34-D4C0C8B3E3F4/WebQSP.zip\n",
    "unzip WebQSP.zip -d data/webqsp/\n",
    "\n",
    "# CWQ (ComplexWebQuestions)\n",
    "wget https://www.tau-nlp.org/compwebq/ComplexWebQuestions_1.1.zip\n",
    "unzip ComplexWebQuestions_1.1.zip -d data/cwq/\n",
    "\n",
    "# MetaQA\n",
    "git clone https://github.com/yuyuz/MetaQA.git data/metaqa/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with full dataset (uncomment when you have the data)\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "\n",
    "# Load full WebQSP dataset\n",
    "webqsp_full = WebQSP(data_dir=Path(\"data/webqsp\"))\n",
    "print(f\"Full WebQSP: {len(webqsp_full)} questions\")\n",
    "\n",
    "# Run with larger subset\n",
    "config = RunConfig(\n",
    "    subset_size=500,  # Or None for all\n",
    "    random_seed=42,\n",
    ")\n",
    "\n",
    "report = runner.evaluate(webqsp_full, config=config)\n",
    "print(report.summary())\n",
    "\"\"\"\n",
    "print(\"üí° Uncomment the code above after downloading the datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Summary\n",
    "\n",
    "This notebook covered:\n",
    "\n",
    "1. **Available benchmarks**: WebQSP, CWQ, MetaQA, SimpleQuestions\n",
    "2. **Metrics**: Exact Match (EM), F1 Score, answer normalization\n",
    "3. **BenchmarkRunner**: Automated evaluation with reporting\n",
    "4. **MetricsCalculator**: Manual evaluation for custom datasets\n",
    "5. **Error analysis**: Understanding model failures\n",
    "6. **Result persistence**: Saving reports for later analysis\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Download full benchmark datasets for comprehensive evaluation\n",
    "- Compare different reasoners on the same benchmark\n",
    "- Analyze performance by question type (1-hop vs multi-hop)\n",
    "- Use with real LLMs for better performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
