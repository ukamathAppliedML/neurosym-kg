{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuroSym-KG: Quick Start Guide\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-org/neurosym-kg/blob/main/notebooks/01_quickstart.ipynb)\n",
    "\n",
    "This notebook introduces the NeuroSym-KG framework for neuro-symbolic reasoning over knowledge graphs.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to install NeuroSym-KG\n",
    "- Creating your first knowledge graph\n",
    "- Running a reasoning query\n",
    "- Understanding the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "\n",
    "First, let's install NeuroSym-KG and its dependencies. This cell only needs to run once per Colab session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install NeuroSym-KG from GitHub\n",
    "!pip install -q git+https://github.com/your-org/neurosym-kg.git\n",
    "\n",
    "# Or if you have the package locally/uploaded:\n",
    "# !pip install -q neurosym-kg\n",
    "\n",
    "# For this demo, let's install the core dependencies directly:\n",
    "!pip install -q pydantic pydantic-settings httpx tenacity networkx numpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running from source (e.g., uploaded to Colab), add to path:\n",
    "import sys\n",
    "sys.path.insert(0, '/content/neurosym-kg')  # Adjust path as needed\n",
    "\n",
    "# Verify installation\n",
    "try:\n",
    "    import neurosym_kg\n",
    "    print(f\"âœ… NeuroSym-KG v{neurosym_kg.__version__} installed successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Installation issue: {e}\")\n",
    "    print(\"Please run the installation cell above first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Up API Keys (Optional)\n",
    "\n",
    "If you want to use OpenAI or other LLM providers, set up your API keys.\n",
    "\n",
    "**Recommended**: Use Colab's Secrets feature (ðŸ”‘ icon in left sidebar) to store keys securely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Option 1: Use Colab Secrets (recommended)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"âœ… API key loaded from Colab Secrets\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Option 2: Set directly (not recommended for shared notebooks)\n",
    "# os.environ['OPENAI_API_KEY'] = 'sk-your-key-here'\n",
    "\n",
    "# For this quickstart, we'll use a Mock LLM that doesn't need an API key\n",
    "print(\"â„¹ï¸ This tutorial uses MockLLM - no API key required!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Your First Knowledge Graph\n",
    "\n",
    "Let's create a simple knowledge graph about scientists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neurosym_kg import InMemoryKG, Triple\n",
    "\n",
    "# Create an in-memory knowledge graph\n",
    "kg = InMemoryKG(name=\"Scientists KG\")\n",
    "\n",
    "# Add facts as triples (subject, predicate, object)\n",
    "kg.add_triples([\n",
    "    # Albert Einstein\n",
    "    Triple(\"Albert_Einstein\", \"born_in\", \"Ulm\"),\n",
    "    Triple(\"Albert_Einstein\", \"field\", \"Physics\"),\n",
    "    Triple(\"Albert_Einstein\", \"nationality\", \"German\"),\n",
    "    Triple(\"Albert_Einstein\", \"award\", \"Nobel_Prize_Physics_1921\"),\n",
    "    Triple(\"Albert_Einstein\", \"known_for\", \"Theory_of_Relativity\"),\n",
    "    \n",
    "    # Locations\n",
    "    Triple(\"Ulm\", \"located_in\", \"Germany\"),\n",
    "    Triple(\"Ulm\", \"type\", \"City\"),\n",
    "    \n",
    "    # Marie Curie\n",
    "    Triple(\"Marie_Curie\", \"born_in\", \"Warsaw\"),\n",
    "    Triple(\"Marie_Curie\", \"field\", \"Physics\"),\n",
    "    Triple(\"Marie_Curie\", \"field\", \"Chemistry\"),\n",
    "    Triple(\"Marie_Curie\", \"nationality\", \"Polish\"),\n",
    "    Triple(\"Marie_Curie\", \"award\", \"Nobel_Prize_Physics_1903\"),\n",
    "    Triple(\"Marie_Curie\", \"award\", \"Nobel_Prize_Chemistry_1911\"),\n",
    "    \n",
    "    # More locations\n",
    "    Triple(\"Warsaw\", \"located_in\", \"Poland\"),\n",
    "    Triple(\"Germany\", \"type\", \"Country\"),\n",
    "    Triple(\"Poland\", \"type\", \"Country\"),\n",
    "])\n",
    "\n",
    "# Print summary\n",
    "print(kg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore the Knowledge Graph\n",
    "\n",
    "Let's query our knowledge graph directly before using a reasoner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about Einstein\n",
    "print(\"=\" * 50)\n",
    "print(\"Facts about Einstein:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "einstein_facts = kg.get_neighbors(\"Albert_Einstein\", direction=\"outgoing\")\n",
    "for fact in einstein_facts:\n",
    "    print(f\"  {fact.to_text()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Who works in Physics?\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "physics_triples = kg.get_triples(predicate=\"field\", obj=\"Physics\")\n",
    "for t in physics_triples:\n",
    "    print(f\"  {t.subject_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find paths between entities\n",
    "print(\"Path from Einstein to Germany:\")\n",
    "paths = kg.find_paths(\"Albert_Einstein\", \"Germany\", max_hops=2)\n",
    "\n",
    "for i, path in enumerate(paths):\n",
    "    print(f\"\\nPath {i+1}:\")\n",
    "    for triple in path:\n",
    "        print(f\"  â†’ {triple.to_text()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Set Up the Reasoner\n",
    "\n",
    "Now let's create a reasoner that combines the KG with an LLM to answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neurosym_kg import MockLLMBackend, ThinkOnGraphReasoner\n",
    "\n",
    "# Create a mock LLM for demonstration\n",
    "# In production, you'd use OpenAIBackend or AnthropicBackend\n",
    "llm = MockLLMBackend(default_response=\"Unknown\")\n",
    "\n",
    "# Configure the mock LLM with some responses\n",
    "llm.add_response(r\".*Extract.*entities.*Einstein.*\", \"Albert_Einstein\")\n",
    "llm.add_response(r\".*Extract.*entities.*Curie.*\", \"Marie_Curie\")\n",
    "llm.add_response(r\".*Extract.*entities.*born.*\", \"Albert_Einstein\")\n",
    "llm.add_response(r\".*relevant relations.*\", \"born_in\\nfield\")\n",
    "llm.add_response(r\".*enough information.*\", \"YES\")\n",
    "llm.add_response(r\".*answer.*born.*\", \"Albert Einstein was born in Ulm, Germany.\")\n",
    "llm.add_response(r\".*answer.*field.*\", \"Albert Einstein worked in Physics.\")\n",
    "\n",
    "# Create the Think-on-Graph reasoner\n",
    "reasoner = ThinkOnGraphReasoner(\n",
    "    kg=kg,\n",
    "    llm=llm,\n",
    "    max_depth=2,      # Maximum hops in the graph\n",
    "    beam_width=3,     # Number of paths to explore\n",
    "    verbose=True      # Print debug info\n",
    ")\n",
    "\n",
    "print(\"âœ… Reasoner initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ask Questions!\n",
    "\n",
    "Now let's use the reasoner to answer questions about our knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question\n",
    "question = \"Where was Einstein born?\"\n",
    "print(f\"â“ Question: {question}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "result = reasoner.reason(question)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ðŸ“ Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Answer: {result.answer}\")\n",
    "print(f\"Confidence: {result.confidence:.2%}\")\n",
    "print(f\"Status: {result.status.value}\")\n",
    "print(f\"Latency: {result.latency_ms:.0f}ms\")\n",
    "\n",
    "if result.paths:\n",
    "    print(f\"\\nReasoning Paths ({len(result.paths)} found):\")\n",
    "    for i, path in enumerate(result.paths[:3]):\n",
    "        print(f\"  Path {i+1}: {path.to_text()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try another question\n",
    "question2 = \"What field did Einstein work in?\"\n",
    "print(f\"â“ Question: {question2}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "result2 = reasoner.reason(question2)\n",
    "\n",
    "print(f\"\\nðŸ“ Answer: {result2.answer}\")\n",
    "print(f\"   Confidence: {result2.confidence:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Check Reasoner Statistics\n",
    "\n",
    "The reasoner tracks usage statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = reasoner.stats\n",
    "\n",
    "print(\"ðŸ“Š Reasoner Statistics\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Total queries: {stats['queries']}\")\n",
    "print(f\"KG calls: {stats['kg_calls']}\")\n",
    "print(f\"LLM calls: {stats['llm_calls']}\")\n",
    "print(f\"Successful: {stats['successful']}\")\n",
    "print(f\"Failed: {stats['failed']}\")\n",
    "\n",
    "if stats['queries'] > 0:\n",
    "    print(f\"Avg latency: {stats['total_latency_ms']/stats['queries']:.0f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Using with Real LLM (OpenAI)\n",
    "\n",
    "To use with a real LLM, just swap the backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if you have an OpenAI API key set up\n",
    "\"\"\"\n",
    "from neurosym_kg import OpenAIBackend\n",
    "\n",
    "# Create real LLM backend\n",
    "real_llm = OpenAIBackend(\n",
    "    model=\"gpt-4o-mini\",  # or \"gpt-4o\" for better results\n",
    "    temperature=0.0       # Deterministic outputs\n",
    ")\n",
    "\n",
    "# Create reasoner with real LLM\n",
    "real_reasoner = ThinkOnGraphReasoner(\n",
    "    kg=kg,\n",
    "    llm=real_llm,\n",
    "    max_depth=3,\n",
    "    beam_width=5,\n",
    ")\n",
    "\n",
    "# Ask a question\n",
    "result = real_reasoner.reason(\"Where was Einstein born?\")\n",
    "print(f\"Answer: {result.answer}\")\n",
    "\"\"\"\n",
    "print(\"ðŸ’¡ Uncomment the code above to use with OpenAI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Next Steps\n",
    "\n",
    "Congratulations! You've learned the basics of NeuroSym-KG. Here's what to explore next:\n",
    "\n",
    "1. **[02_comparing_reasoners.ipynb](./02_comparing_reasoners.ipynb)** - Compare different reasoning paradigms\n",
    "2. **[03_custom_kg.ipynb](./03_custom_kg.ipynb)** - Build domain-specific knowledge graphs\n",
    "3. **[04_evaluation.ipynb](./04_evaluation.ipynb)** - Run benchmarks and evaluate performance\n",
    "4. **[05_symbolic_reasoning.ipynb](./05_symbolic_reasoning.ipynb)** - Add rule-based constraints\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [GitHub Repository](https://github.com/your-org/neurosym-kg)\n",
    "- [API Documentation](https://neurosym-kg.readthedocs.io)\n",
    "- [Research Papers](#) (ToG, RoG, GraphRAG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
