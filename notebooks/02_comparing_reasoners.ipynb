{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Reasoning Paradigms\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-org/neurosym-kg/blob/main/notebooks/02_comparing_reasoners.ipynb)\n",
    "\n",
    "This notebook compares different neuro-symbolic reasoning paradigms:\n",
    "\n",
    "| Reasoner | Paper | Approach |\n",
    "|----------|-------|----------|\n",
    "| **Think-on-Graph (ToG)** | ICLR 2024 | LLM as agent with beam search |\n",
    "| **Reasoning on Graphs (RoG)** | ICLR 2024 | Plan-based faithful reasoning |\n",
    "| **GraphRAG** | Microsoft 2024 | Community-based retrieval |\n",
    "| **SubgraphRAG** | 2024 | Flexible subgraph retrieval |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "!pip install -q pydantic pydantic-settings httpx tenacity networkx numpy tqdm\n",
    "\n",
    "# Clone repo if needed (uncomment)\n",
    "# !git clone https://github.com/your-org/neurosym-kg.git\n",
    "# import sys; sys.path.insert(0, 'neurosym-kg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from typing import Dict, List\n",
    "\n",
    "# Import framework\n",
    "from neurosym_kg import (\n",
    "    InMemoryKG,\n",
    "    Triple,\n",
    "    MockLLMBackend,\n",
    ")\n",
    "from neurosym_kg.reasoners import (\n",
    "    ThinkOnGraphReasoner,\n",
    "    ReasoningOnGraphs,\n",
    "    GraphRAGReasoner,\n",
    "    SubgraphRAGReasoner,\n",
    ")\n",
    "from neurosym_kg.evaluation import exact_match, f1_score\n",
    "\n",
    "print(\"âœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Test Knowledge Graph\n",
    "\n",
    "We'll use a movie knowledge graph for this comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_movie_kg() -> InMemoryKG:\n",
    "    \"\"\"Create a movie knowledge graph for testing.\"\"\"\n",
    "    kg = InMemoryKG(name=\"Movies KG\")\n",
    "    \n",
    "    kg.add_triples([\n",
    "        # Inception\n",
    "        Triple(\"Inception\", \"director\", \"Christopher_Nolan\"),\n",
    "        Triple(\"Inception\", \"year\", \"2010\"),\n",
    "        Triple(\"Inception\", \"genre\", \"Science_Fiction\"),\n",
    "        Triple(\"Inception\", \"starring\", \"Leonardo_DiCaprio\"),\n",
    "        Triple(\"Inception\", \"starring\", \"Tom_Hardy\"),\n",
    "        Triple(\"Inception\", \"starring\", \"Ellen_Page\"),\n",
    "        Triple(\"Inception\", \"budget\", \"160_million_USD\"),\n",
    "        \n",
    "        # The Dark Knight\n",
    "        Triple(\"The_Dark_Knight\", \"director\", \"Christopher_Nolan\"),\n",
    "        Triple(\"The_Dark_Knight\", \"year\", \"2008\"),\n",
    "        Triple(\"The_Dark_Knight\", \"genre\", \"Action\"),\n",
    "        Triple(\"The_Dark_Knight\", \"starring\", \"Christian_Bale\"),\n",
    "        Triple(\"The_Dark_Knight\", \"starring\", \"Heath_Ledger\"),\n",
    "        \n",
    "        # Interstellar\n",
    "        Triple(\"Interstellar\", \"director\", \"Christopher_Nolan\"),\n",
    "        Triple(\"Interstellar\", \"year\", \"2014\"),\n",
    "        Triple(\"Interstellar\", \"genre\", \"Science_Fiction\"),\n",
    "        Triple(\"Interstellar\", \"starring\", \"Matthew_McConaughey\"),\n",
    "        \n",
    "        # Director info\n",
    "        Triple(\"Christopher_Nolan\", \"born_in\", \"London\"),\n",
    "        Triple(\"Christopher_Nolan\", \"nationality\", \"British\"),\n",
    "        Triple(\"Christopher_Nolan\", \"occupation\", \"Director\"),\n",
    "        Triple(\"Christopher_Nolan\", \"birth_year\", \"1970\"),\n",
    "        \n",
    "        # Actor info\n",
    "        Triple(\"Leonardo_DiCaprio\", \"born_in\", \"Los_Angeles\"),\n",
    "        Triple(\"Leonardo_DiCaprio\", \"occupation\", \"Actor\"),\n",
    "        Triple(\"Leonardo_DiCaprio\", \"award\", \"Academy_Award_2016\"),\n",
    "        \n",
    "        Triple(\"Tom_Hardy\", \"born_in\", \"London\"),\n",
    "        Triple(\"Tom_Hardy\", \"occupation\", \"Actor\"),\n",
    "        \n",
    "        # Titanic (different director)\n",
    "        Triple(\"Titanic\", \"director\", \"James_Cameron\"),\n",
    "        Triple(\"Titanic\", \"year\", \"1997\"),\n",
    "        Triple(\"Titanic\", \"starring\", \"Leonardo_DiCaprio\"),\n",
    "        Triple(\"Titanic\", \"starring\", \"Kate_Winslet\"),\n",
    "        Triple(\"Titanic\", \"genre\", \"Drama\"),\n",
    "        \n",
    "        # Location hierarchy\n",
    "        Triple(\"London\", \"located_in\", \"United_Kingdom\"),\n",
    "        Triple(\"Los_Angeles\", \"located_in\", \"California\"),\n",
    "        Triple(\"California\", \"located_in\", \"United_States\"),\n",
    "    ])\n",
    "    \n",
    "    return kg\n",
    "\n",
    "kg = create_movie_kg()\n",
    "print(kg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Test Questions\n",
    "\n",
    "We'll test with questions of varying complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test questions with ground truth answers\n",
    "TEST_QUESTIONS = [\n",
    "    # 1-hop questions (direct lookup)\n",
    "    {\n",
    "        \"question\": \"Who directed Inception?\",\n",
    "        \"answers\": [\"Christopher Nolan\", \"Christopher_Nolan\"],\n",
    "        \"type\": \"1-hop\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What year was The Dark Knight released?\",\n",
    "        \"answers\": [\"2008\"],\n",
    "        \"type\": \"1-hop\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What genre is Interstellar?\",\n",
    "        \"answers\": [\"Science Fiction\", \"Science_Fiction\", \"Sci-Fi\"],\n",
    "        \"type\": \"1-hop\",\n",
    "    },\n",
    "    \n",
    "    # 2-hop questions (require traversal)\n",
    "    {\n",
    "        \"question\": \"Where was the director of Inception born?\",\n",
    "        \"answers\": [\"London\"],\n",
    "        \"type\": \"2-hop\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the nationality of the director of The Dark Knight?\",\n",
    "        \"answers\": [\"British\"],\n",
    "        \"type\": \"2-hop\",\n",
    "    },\n",
    "    \n",
    "    # Aggregation questions\n",
    "    {\n",
    "        \"question\": \"What movies did Christopher Nolan direct?\",\n",
    "        \"answers\": [\"Inception\", \"The Dark Knight\", \"Interstellar\"],\n",
    "        \"type\": \"aggregation\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Created {len(TEST_QUESTIONS)} test questions:\")\n",
    "for i, q in enumerate(TEST_QUESTIONS, 1):\n",
    "    print(f\"  {i}. [{q['type']}] {q['question']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Mock LLM\n",
    "\n",
    "For reproducible comparisons, we use a mock LLM with predefined responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mock_llm() -> MockLLMBackend:\n",
    "    \"\"\"Create a mock LLM with responses for our movie questions.\"\"\"\n",
    "    llm = MockLLMBackend(default_response=\"Unknown\")\n",
    "    \n",
    "    # Entity extraction patterns\n",
    "    llm.add_response(r\".*Extract.*Inception.*\", \"Inception\")\n",
    "    llm.add_response(r\".*Extract.*Dark Knight.*\", \"The_Dark_Knight\")\n",
    "    llm.add_response(r\".*Extract.*Interstellar.*\", \"Interstellar\")\n",
    "    llm.add_response(r\".*Extract.*Nolan.*\", \"Christopher_Nolan\")\n",
    "    llm.add_response(r\".*Extract.*director.*Inception.*\", \"Inception\")\n",
    "    llm.add_response(r\".*Extract.*movies.*Nolan.*\", \"Christopher_Nolan\")\n",
    "    \n",
    "    # Relation selection\n",
    "    llm.add_response(r\".*relations.*director.*\", \"director\")\n",
    "    llm.add_response(r\".*relations.*year.*\", \"year\")\n",
    "    llm.add_response(r\".*relations.*genre.*\", \"genre\")\n",
    "    llm.add_response(r\".*relations.*born.*\", \"born_in\")\n",
    "    llm.add_response(r\".*relations.*nationality.*\", \"nationality\")\n",
    "    llm.add_response(r\".*relations.*movie.*\", \"director\")\n",
    "    \n",
    "    # Plan generation for RoG\n",
    "    llm.add_response(r\".*relation paths.*director.*born.*\", \"director -> born_in\")\n",
    "    llm.add_response(r\".*relation paths.*director.*nationality.*\", \"director -> nationality\")\n",
    "    llm.add_response(r\".*relation paths.*director.*\", \"director\")\n",
    "    \n",
    "    # Reasoning checks\n",
    "    llm.add_response(r\".*enough information.*\", \"YES - Found relevant facts.\")\n",
    "    \n",
    "    # Answer generation\n",
    "    llm.add_response(r\".*answer.*director.*Inception.*\", \"Christopher Nolan directed Inception.\")\n",
    "    llm.add_response(r\".*answer.*year.*Dark Knight.*\", \"The Dark Knight was released in 2008.\")\n",
    "    llm.add_response(r\".*answer.*genre.*Interstellar.*\", \"Interstellar is a Science Fiction film.\")\n",
    "    llm.add_response(r\".*answer.*born.*director.*Inception.*\", \"Christopher Nolan was born in London.\")\n",
    "    llm.add_response(r\".*answer.*nationality.*director.*\", \"Christopher Nolan is British.\")\n",
    "    llm.add_response(r\".*answer.*movies.*Nolan.*\", \"Christopher Nolan directed Inception, The Dark Knight, and Interstellar.\")\n",
    "    \n",
    "    return llm\n",
    "\n",
    "print(\"âœ… Mock LLM factory created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize All Reasoners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all reasoners\n",
    "reasoners = {\n",
    "    \"Think-on-Graph (ToG)\": ThinkOnGraphReasoner(\n",
    "        kg=kg,\n",
    "        llm=create_mock_llm(),\n",
    "        max_depth=3,\n",
    "        beam_width=5,\n",
    "        verbose=False,\n",
    "    ),\n",
    "    \"Reasoning on Graphs (RoG)\": ReasoningOnGraphs(\n",
    "        kg=kg,\n",
    "        llm=create_mock_llm(),\n",
    "        max_path_length=3,\n",
    "        num_plans=3,\n",
    "        verbose=False,\n",
    "    ),\n",
    "    \"GraphRAG\": GraphRAGReasoner(\n",
    "        kg=kg,\n",
    "        llm=create_mock_llm(),\n",
    "        verbose=False,\n",
    "    ),\n",
    "    \"SubgraphRAG\": SubgraphRAGReasoner(\n",
    "        kg=kg,\n",
    "        llm=create_mock_llm(),\n",
    "        subgraph_size=30,\n",
    "        verbose=False,\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(f\"âœ… Initialized {len(reasoners)} reasoners:\")\n",
    "for name in reasoners:\n",
    "    print(f\"   â€¢ {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_reasoner(reasoner, questions: List[Dict]) -> Dict:\n",
    "    \"\"\"Evaluate a reasoner on a set of questions.\"\"\"\n",
    "    results = {\n",
    "        \"correct\": 0,\n",
    "        \"total\": len(questions),\n",
    "        \"total_latency_ms\": 0,\n",
    "        \"details\": [],\n",
    "    }\n",
    "    \n",
    "    for q in questions:\n",
    "        try:\n",
    "            result = reasoner.reason(q[\"question\"])\n",
    "            \n",
    "            # Calculate metrics\n",
    "            em = exact_match(result.answer, q[\"answers\"])\n",
    "            f1 = f1_score(result.answer, q[\"answers\"])\n",
    "            \n",
    "            results[\"correct\"] += em\n",
    "            results[\"total_latency_ms\"] += result.latency_ms\n",
    "            \n",
    "            results[\"details\"].append({\n",
    "                \"question\": q[\"question\"],\n",
    "                \"answer\": result.answer,\n",
    "                \"expected\": q[\"answers\"],\n",
    "                \"exact_match\": em,\n",
    "                \"f1\": f1,\n",
    "                \"latency_ms\": result.latency_ms,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            results[\"details\"].append({\n",
    "                \"question\": q[\"question\"],\n",
    "                \"error\": str(e),\n",
    "            })\n",
    "    \n",
    "    results[\"accuracy\"] = results[\"correct\"] / results[\"total\"]\n",
    "    results[\"avg_latency_ms\"] = results[\"total_latency_ms\"] / results[\"total\"]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation for all reasoners\n",
    "print(\"Running evaluation...\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for name, reasoner in reasoners.items():\n",
    "    print(f\"\\nðŸ“Š Evaluating: {name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    results = evaluate_reasoner(reasoner, TEST_QUESTIONS)\n",
    "    all_results[name] = results\n",
    "    \n",
    "    print(f\"   Accuracy: {results['accuracy']:.1%}\")\n",
    "    print(f\"   Avg Latency: {results['avg_latency_ms']:.0f}ms\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"                    COMPARISON RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Reasoner':<30} {'Accuracy':>12} {'Avg Latency':>15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, results in all_results.items():\n",
    "    print(f\"{name:<30} {results['accuracy']:>11.1%} {results['avg_latency_ms']:>12.0f}ms\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed per-question results\n",
    "print(\"\\nðŸ“‹ Detailed Results by Question\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, q in enumerate(TEST_QUESTIONS):\n",
    "    print(f\"\\nQ{i+1}: {q['question']}\")\n",
    "    print(f\"    Expected: {q['answers']}\")\n",
    "    print(f\"    Type: {q['type']}\")\n",
    "    print()\n",
    "    \n",
    "    for name, results in all_results.items():\n",
    "        detail = results[\"details\"][i]\n",
    "        if \"error\" in detail:\n",
    "            print(f\"    {name:<25}: âŒ Error - {detail['error'][:30]}\")\n",
    "        else:\n",
    "            status = \"âœ…\" if detail[\"exact_match\"] else \"âŒ\"\n",
    "            print(f\"    {name:<25}: {status} {detail['answer'][:40]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple ASCII bar chart\n",
    "print(\"\\nðŸ“Š Accuracy Comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "max_bar = 40\n",
    "for name, results in all_results.items():\n",
    "    acc = results['accuracy']\n",
    "    bar_len = int(acc * max_bar)\n",
    "    bar = \"â–ˆ\" * bar_len + \"â–‘\" * (max_bar - bar_len)\n",
    "    print(f\"{name:<25}\")\n",
    "    print(f\"  [{bar}] {acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analysis by Question Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance by question type\n",
    "question_types = set(q[\"type\"] for q in TEST_QUESTIONS)\n",
    "\n",
    "print(\"\\nðŸ“ˆ Performance by Question Type\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for qtype in question_types:\n",
    "    print(f\"\\n{qtype.upper()}:\")\n",
    "    \n",
    "    # Get indices for this question type\n",
    "    indices = [i for i, q in enumerate(TEST_QUESTIONS) if q[\"type\"] == qtype]\n",
    "    \n",
    "    for name, results in all_results.items():\n",
    "        correct = sum(1 for i in indices if results[\"details\"][i].get(\"exact_match\", 0))\n",
    "        total = len(indices)\n",
    "        acc = correct / total if total > 0 else 0\n",
    "        print(f\"  {name:<30}: {correct}/{total} ({acc:.0%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Using with Real LLM\n",
    "\n",
    "To compare with a real LLM (better results, costs money):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to use with OpenAI\n",
    "\"\"\"\n",
    "from neurosym_kg import OpenAIBackend\n",
    "\n",
    "real_llm = OpenAIBackend(model=\"gpt-4o-mini\")\n",
    "\n",
    "real_reasoners = {\n",
    "    \"ToG + GPT-4o-mini\": ThinkOnGraphReasoner(kg=kg, llm=real_llm),\n",
    "    \"RoG + GPT-4o-mini\": ReasoningOnGraphs(kg=kg, llm=real_llm),\n",
    "}\n",
    "\n",
    "for name, reasoner in real_reasoners.items():\n",
    "    print(f\"\\nTesting {name}...\")\n",
    "    result = reasoner.reason(\"Who directed Inception?\")\n",
    "    print(f\"Answer: {result.answer}\")\n",
    "\"\"\"\n",
    "print(\"ðŸ’¡ Uncomment the code above to compare with real LLM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "1. **Think-on-Graph (ToG)**: Best for exploratory queries, finds multiple reasoning paths\n",
    "2. **Reasoning on Graphs (RoG)**: Best for structured multi-hop queries, provides faithful paths\n",
    "3. **GraphRAG**: Best for global/aggregation queries over large KGs\n",
    "4. **SubgraphRAG**: Good balance, flexible subgraph sizing\n",
    "\n",
    "### When to Use Each:\n",
    "\n",
    "| Use Case | Recommended Reasoner |\n",
    "|----------|----------------------|\n",
    "| Simple 1-hop queries | Any (SubgraphRAG fastest) |\n",
    "| Multi-hop reasoning | ToG or RoG |\n",
    "| Aggregation queries | GraphRAG |\n",
    "| Limited compute | SubgraphRAG |\n",
    "| Explainability needed | RoG (faithful paths) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
